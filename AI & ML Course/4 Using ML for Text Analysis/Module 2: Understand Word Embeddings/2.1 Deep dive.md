# üìö Introduction to Word Embeddings

Another common approach for converting text into numerical features is through **word embeddings**.

Word embeddings are vector representations of words that capture their **semantic meaning**. They are generated by analyzing large amounts of text and learning **probabilistic relationships** between words and their surrounding context.

---

## üé• Overview

In this video, **Mr. D'Alessandro** introduces:

- The concept of **word embeddings** and their significance in preserving the meaning behind words.
- How embeddings can be used **off-the-shelf** to convert text into numeric representations.
- A demonstration of computing **semantic similarity** between words using **cosine similarity**.

---

## üí° Key Points

- Word embeddings retain **semantic relationships** (e.g., "king" - "man" + "woman" ‚âà "queen").
- They are typically trained on large corpora (e.g., Word2Vec, GloVe, FastText).
- **Cosine similarity** is used to measure



The word embeddings technique converts words to numerical vectors. For example, the word ‚Äúdoctor‚Äù may be mapped to a vector of 
. Word embeddings are similar to vectorizers, such as TF-IDF, in that they both convert text into numerical representations. The difference between the two is that word embeddings seek to capture the meanings of words within a body of text, while vectorizers simply convert words into numbers based on some predetermined rules. 

The word embeddings technique essentially views words with similar meanings as being similar to each other mathematically. The technique learns relationships among words by analyzing a large corpus of text. It then creates numerical vectors by mapping related words in the input text, such as the words "doctor" and "patient," to similar vectors that are closely spaced in vector space. Words that are unrelated, such as "doctor" and "tree," have dissimilar vectors. 


# Word Embeddings and Cosine Similarity

## Cosine Similarity Formula

To calculate similarity between two words, cosine similarity is most commonly used:

```
Cosine Similarity = (V‚ÇÅ ¬∑ V‚ÇÇ) / (||V‚ÇÅ|| ||V‚ÇÇ||)
```

Where:
- V‚ÇÅ and V‚ÇÇ are vectors of the same dimension
- The numerator is the dot product of the two vectors
- The denominator is the product of the L2 norms of the two vectors

## L2 Norm Definition

The L2 norm of a k-dimensional vector V = (v‚ÇÅ, ..., v‚Çñ) is defined by:

```
||V|| = ‚àö(Œ£·µ¢‚Çå‚ÇÅ·µè v·µ¢¬≤) = ‚àö(v‚ÇÅ¬≤ + ... + v‚Çñ¬≤)
```

## Geometric Interpretation

- The L2 norm ||V|| represents the length of vector V
- Cosine similarity equals the cosine of the angle between vectors V‚ÇÅ and V‚ÇÇ
- **Same direction**: cosine similarity = 1 (0¬∞ angle)
- **Opposite direction**: cosine similarity = -1 (180¬∞ angle)  
- **Orthogonal**: cosine similarity = 0 (90¬∞ angle)

## Word Embeddings Overview

Word embeddings convert words to numerical vectors that capture semantic meanings. Key characteristics:

- **Similar words** ‚Üí similar vectors (closely spaced in vector space)
- **Unrelated words** ‚Üí dissimilar vectors
- Example: "doctor" and "patient" have similar vectors; "doctor" and "tree" have dissimilar vectors

### How Word Embeddings Work

1. Analyze large corpus of text to learn word relationships
2. Map related words to similar vectors in vector space
3. Create numerical representations that preserve semantic meaning
4. Enable machine learning models to understand word relationships

### Interesting Properties

Word embeddings can capture semantic relationships like:
```
W‚Çñ·µ¢‚Çô‚Çò - W‚Çò‚Çê‚Çô + W·µ•‚Çí‚Çò‚Çê‚Çô ‚âà Wq·µ§‚Çë‚Çë‚Çô
```

## Word Embeddings vs TF-IDF

**When to use each approach:**

- **TF-IDF**: Small vocabulary with high-frequency words
- **Word Embeddings**: Large vocabulary with low-frequency words

## Technical Implementation

- Popular technique: **Word2Vec** (created by Google in 2013)
- Requires training on large datasets like any machine learning model
- Results in vectors for each word in the training dictionary
- Can be used by various ML models (logistic regression, random forest, neural networks)

# 🧠 Using Word Embeddings in NLP Models

In a previous lecture, we introduced **word embeddings**—a way to represent words as **K-dimensional vectors**. These embeddings capture semantic meaning, and words with **high cosine similarity** often have similar meanings.

This guide explores how to **apply word embeddings** to model training and highlights the motivation, processing steps, and design choices.

---

## 📌 Why Use Word Embeddings?

Word embeddings offer several key advantages:

- ✅ **Dimensionality Reduction**: They reduce feature counts from hundreds of thousands to typically <300.
- ✅ **Computational Efficiency**: Smaller feature space leads to lower memory and CPU usage.
- ✅ **Less Data Sparsity**: Embeddings allow rare words to share features with similar ones, improving learning from limited data.

---

## 🔄 From Word Embeddings to Model Input

Text examples consist of multiple words. Since each word maps to a K-dimensional vector, we must **aggregate** these vectors into a single, fixed-length vector for model training.

### 🧩 Aggregation Methods (Pooling Layers)

| Method       | Description                                                                 |
|--------------|-----------------------------------------------------------------------------|
| **Average**  | Compute element-wise average of all word vectors.                          |
| **Max**      | Takes the maximum value in each vector dimension—captures standout signals. |
| **Min**      | Captures the smallest values—less commonly used.                            |

> 🔄 These are referred to as **pooling layers** in the embedding pipeline.

---

## 📈 Weighted Aggregation with TF-IDF

Not all words contribute equally. To emphasize more meaningful words:

1. **Compute TF-IDF** scores for words in a document.
2. **Multiply each word vector** by its TF-IDF weight.
3. **Apply pooling (e.g., average, max)** on the weighted vectors.

This technique reduces the influence of frequent, generic words and amplifies rare but informative ones.

---

## 🧬 Dual Pooling (Average + Max)

To capture a **richer semantic representation**:

- Use **both average and max pooling**.
- **Concatenate** the resulting vectors.
- Output dimension becomes **2 × K**.

This approach balances **general context** (average) with **strong signals** (max).

---

## 📦 Choosing an Embedding Model

Several pretrained word embeddings are available:

| Model        | Description                      |
|--------------|----------------------------------|
| **Word2Vec** | Trained using context windows    |
| **GloVe**    | Based on word co-occurrence      |
| **FastText** | Includes subword information     |

### 🎯 Consider:
- Whether embeddings are appropriate for your task.
- Pretrained vs. custom-trained embeddings.
- Embedding size: commonly **50**, **100**, or **300** dimensions.

---

## 🔁 Best Practices for Using Word Embeddings

1. **Start simple** with one embedding + basic pooling.
2. **Compare** performance against non-embedding methods.
3. **Iterate**: Adjust embedding size, pooling function, or add weights.
4. **Validate empirically** using model performance metrics.

> 🧪 There’s no one-size-fits-all — experiment and tune based on your specific use case.

---

## ✅ Summary

Word embeddings offer a powerful way to represent language numerically while preserving meaning. By leveraging **pooling strategies** and **weighting mechanisms**, you can enhance your model’s understanding of text in a compact and semantically rich form.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Text into Features Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, you will see how to train a Word2Vec model to obtain word embeddings for spam message classification. We will train a logistic regression model using the embeddings as features. We will then use our model to predict whether a new email is spam.\n",
    "\n",
    "A Word2Vec model should be trained on a large text corpus that is relevant to the domain in which you will later use a machine learning model to make predictions. For example, if you will be working with medical data, your Word2Vec model should not be trained on text data originating from product reviews, but rather should be trained on medical data.\n",
    "\n",
    "You can train a Word2Vec model using an existing tool, such as Gensim, which is a Python package used for natural language processing. After the embeddings have been produced, you will use them to create feature vectors for your machine learning model. For example, we can represent each training example as a feature vector by taking the average of the Word2Vec embeddings of the words in that training example. Once we have created feature vectors for all of our training examples, we can fit our machine learning model to the training data and use our trained model to make predictions. \n",
    "\n",
    "This demo we will walk you through the following steps:\n",
    "\n",
    "1. Load the spam message data set. The spam email dataset contains email subject lines.\n",
    "\n",
    "2. Create labeled examples out of the data: We will have one text feature and one label. Each feature will contain one email subject line.\n",
    "\n",
    "3. Preprocess the text features: We will preprocess the data by removing stop words, converting all text to lowercase, removing punctuation, etc. from the email subject lines. \n",
    "\n",
    "4. Create training and test datasets: Each example in the training and test datasets will now contain one preprocessed email subject line as a feature.\n",
    "\n",
    "5. Train the Word2Vec model using the training dataset's features: we will train a Word2Vec model using Gensim. The model will figure out how to represent words as vectors based on how they are used in the spam email dataset. You will inspect the resulting word embeddings to develop a better understanding of what they can tell you about a particular word.\n",
    "\n",
    "6. Create feature vectors out of the training and test data: After training the Word2Vec model, we will represent the features in every training and test example (recall, each example contains an email subject line) as a vector by taking the average of the Word2Vec embeddings of the words in the example. \n",
    "\n",
    "7. Train a logistic regression model using the feature vectors as input features, and evaluate the model's performance.\n",
    "\n",
    "**<font color='red'>Note: Some of the code cells in this notebook may take a while to run.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages. Run the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import the scikit-learn `LogisticRegression`, the `train_test_split()` function for splitting the data into training and test sets, and the function `roc_auc_score` to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load a Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a dataset containing emails that are labeled as either spam or not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"spamDataset.csv\")\n",
    "df = pd.read_csv(filename, header=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Labeled Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create labeled examples from our dataset. We will have one text feature and one label. \n",
    "The code cell below carries out the following steps:\n",
    "\n",
    "* Gets the `spam` column from DataFrame `df` and assigns it to the variable `y`. This will be our label. Note that the label contains True or False values that indicate whether an email is spam or not spam.\n",
    "* Gets the column `email_text` from DataFrame `df` and assigns it to the variable `X`. This will our feature. Note that the `email_text` feature contains the subject line of the email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['spam'] \n",
    "X = df['email_text']\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example of a spam email and an email that is not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A spam email: \\n\\n', X[67])\n",
    "print('A non-spam email: \\n\\n', X[135])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to preprocess the text. Preprocessing technqiues can include cleaning the data, converting all text to lowercase, removing special characters, removing stopwords from the text, tokenizing the text (split it into smaller chunks), and lemmatizing the text (converting a word to its root word).\n",
    "\n",
    "You can perform preprocessing on your own or use tools to accomplish this. One common tool is NLTK [Natural Language Toolkit](https://www.nltk.org/). However, for this demo, we will use the built-in function from Gensim to preprocess the text. This function will remove some stop words, covert all text to lowercase, remove punctuation and tokenize the text.\n",
    "\n",
    "Let's import the Gensim package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we preprocess our data, let's look at an example output of preprocessed text. Let's take a simple sentence and perform preprocessing. Run the code cell below and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I went to the market to buy some apples for my pet horse.\" \n",
    "\n",
    "list(gensim.utils.simple_preprocess(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform preprocessing on our `email_text` feature and compare the difference between the original text and the preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_X = X\n",
    "X = X.apply(lambda row: gensim.utils.simple_preprocess(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Training and Test Data Sets\n",
    "\n",
    "Let's take our preprocessed text data set and split the data into training and test sets with 80% of the data being the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.80, random_state=1234)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training the Word2Vec Model and Inspecting the Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been preprocessed and we have our training data, we will train a Word2Vec model using Gensim using the training data `X_train`. For more information on Gensim, consult the online [documentation](https://radimrehurek.com/gensim/models/word2vec.html). The model will produce word embeddings. That is, words represented as numerical vectors based on how these words are used in the email spam dataset.\n",
    "    \n",
    "We will specify the following parameters:\n",
    "\n",
    "* size: dimension of each resulting vector that will represent each word.\n",
    "* window: the number of words behind or ahead of a target word that will be used to provide context for that word\n",
    "* min_count: the number of times a word must appear in our text document in order to create a word vector. The model will ignore words that do not satisfy the `min_count` specification, therefore ignoring wors that are not important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin\")\n",
    "word2vec_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=2)\n",
    "\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the size of a particular model with the `len()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2vec_model.wv.key_to_index)  # retrieve vocabulary and measure its size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While 18,015 words may seem like a lot, this is actually quite small for most real-world applications. In practice, you can use models with larger vector sizes (say 300) and much larger vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if a word is included the vocabulary, you can use the `key_to_index` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'cornell' in word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'dog' in word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word `'dog'` is in our vocabulary. Let's inspect its word vector. The code cell belows retrieves the vector for the word `'dog'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the vector, noting each of the following important characteristics: \n",
    "\n",
    "1. It contains 100 values somewhere between -0.39333 and 0.4495\n",
    "2. There are no zeros, so it can be considered a **dense vector** (a vector comprised of mostly non-zero values)\n",
    "3. Each value represents a dimension, but is not necessarily interpretable by humans\n",
    "4. Larger (in magnitude) values may relate to categories in which `'dog'` has a presence\n",
    "5. Many broader categories are also likely to be represented by a combination of these dimensions\n",
    "6. A few values are relatively close to zero. This implies that these dimensions do not carry \"significant information\" about `'dog'`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most similar words for `'dog'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below outputs the first few 25 words that our Word2Vec model learned a vector for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top25 = word2vec_model.wv.index_to_key[:25]\n",
    "top25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go a step further and print vectors associated with each of these words. Use the `background_gradient` method to wrap these vectors into a DataFrame as rows with the corresponding words as row indices. Let's spice it up with colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({w:word2vec_model.wv[w] for w in top25}).T.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 100 columns you see here are dimensions you specified earlier. We do not know what each dimension signifies, although this is an active area of research. However, if you examine each dimension you'll notice more red in some columns and more blue in others. Some of the words are very common and might be stop words we would normally remove. However, if we load more specific groups of words, we might make good guesses about what some of the dimensions mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cells below compute the similarities between two words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.similarity(w1='dog', w2='person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.similarity(w1='dog', w2='cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Feature Vectors out of Word Embeddings for a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert the features in our training and test datasets into feature vectors using our word embeddings. We will use this  to train a logistic regression model. Let's first inspect our original training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below walks through every example in both the training and test datasets and replaces every word contained in the `email_text` feature with its corresponding word embedding. Original words that do not have a corresponding word embedding will not be part of the training and test sets. For example, stop words that were removed when creating the word embeddings will not appear in the training and test sets.\n",
    "\n",
    "<b>Note</b>: This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(word2vec_model.wv.index_to_key)\n",
    "\n",
    "print('Begin transforming X_train')\n",
    "X_train_word_embeddings = np.array([np.array([word2vec_model.wv[word] for word in words if word in training_example])\n",
    "                        for training_example in X_train], dtype=object)\n",
    "print('Finish transforming X_train')\n",
    "\n",
    "print('Begin transforming X_test')\n",
    "X_test_word_embeddings = np.array([np.array([word2vec_model.wv[word] for word in words if word in training_example])\n",
    "                        for training_example in X_test], dtype=object)\n",
    "print('Finish transforming X_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of words in first training example: {0}'.format(len(X_train.iloc[0])))\n",
    "print('First word in first training example: {0}'.format(X_train.iloc[0][0]))\n",
    "print('Second word in first training example: {0}\\n'.format(X_train.iloc[0][1]))\n",
    "\n",
    "print('Number of word vectors in first training example: {0}'.format(len(X_train_word_embeddings[0])))\n",
    "print('First word vector in first training example:\\n {0}'.format(X_train_word_embeddings[0][0]))\n",
    "print('\\nSecond word vector in first training example: \\n {0}\\n'.format(X_train_word_embeddings[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After replacing the `email_text` feature in our training and test data with word embeddings, each example in our training and test data now has a different number of features, each corresponding to a word vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of word vectors in first five examples in training set:')\n",
    "for w in range(0, 5):\n",
    "    print(len(X_train_word_embeddings[w]))\n",
    "\n",
    "print('Number of word vectors in first five examples in test set:')\n",
    "for w in range(0, 5):\n",
    "    print(len(X_test_word_embeddings[w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will cause an error when we train our model. We have to create feature vectors that will provide our classifier with a consistent set of features per example.\n",
    "\n",
    "We can take an element-wise average of the word embeddings of the words contained in each training and test example. This makes a collection of feature vector representations that can be used as training and test features for our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feature_vector = []\n",
    "for w in X_train_word_embeddings:\n",
    "    if w.size:\n",
    "        X_train_feature_vector.append(w.mean(axis=0))\n",
    "    else:\n",
    "        X_train_feature_vector.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_feature_vector = []\n",
    "for w in X_test_word_embeddings:\n",
    "    if w.size:\n",
    "        X_test_feature_vector.append(w.mean(axis=0))\n",
    "    else:\n",
    "        X_test_feature_vector.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example now consists of one feature, which is a numerical feature vector of length 100. Run the code cell below to inspect the first five training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(0, 5):\n",
    "    print('Length of training example {0}: {1}'.format(w, len(X_train_feature_vector[w])))\n",
    "    \n",
    "print('First training example\\'s feature vector: \\n{0}'.format(X_train_feature_vector[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Fit a Logistic Regression Model to the Training Data and Evaluate the Model\n",
    "\n",
    "Now we can train our model on our transformed data. The code cell below trains a logistic regression model and computes the AUC on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a LogisticRegression model object, and fit a Logistic Regression model to the transformed training data\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_feature_vector, y_train)\n",
    "\n",
    "# 2. Make predictions on the transformed test data using the predict_proba() method and \n",
    "# save the values of the second column\n",
    "probability_predictions = model.predict_proba(X_test_feature_vector)[:,1]\n",
    "\n",
    "# 3. Make predictions on the transformed test data using the predict() method \n",
    "class_label_predictions = model.predict(X_test_feature_vector)\n",
    "\n",
    "# 4. Compute the Area Under the ROC curve (AUC) for the test data. Note that this time we are using one \n",
    "# function 'roc_auc_score()' to compute the auc rather than using both 'roc_curve()' and 'auc()' as we have \n",
    "# done in the past\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "print('AUC on the test data: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check two emails and see if our model properly predicted whether an email is spam or not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Email #1:\\n')\n",
    "print(original_X[14])\n",
    "\n",
    "print('\\nPrediction: Is this a spam email? {}\\n'.format(class_label_predictions[14])) \n",
    "\n",
    "print('Actual: Is this a spam email? {}\\n'.format(y_test.to_numpy()[14]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Email #2:\\n')\n",
    "print(original_X[132])\n",
    "\n",
    "print('\\nPrediction: Is this a spam email? {}\\n'.format(class_label_predictions[132])) \n",
    "\n",
    "print('Actual: Is this a good spam email? {}\\n'.format(y_test.to_numpy()[132]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

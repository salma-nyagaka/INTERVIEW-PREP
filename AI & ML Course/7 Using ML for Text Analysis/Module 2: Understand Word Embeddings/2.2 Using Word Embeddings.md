# ðŸ§  Using Word Embeddings in NLP Models

In a previous lecture, we introduced **word embeddings**â€”a way to represent words as **K-dimensional vectors**. These embeddings capture semantic meaning, and words with **high cosine similarity** often have similar meanings.

This guide explores how to **apply word embeddings** to model training and highlights the motivation, processing steps, and design choices.

---

## ðŸ“Œ Why Use Word Embeddings?

Word embeddings offer several key advantages:

- âœ… **Dimensionality Reduction**: They reduce feature counts from hundreds of thousands to typically <300.
- âœ… **Computational Efficiency**: Smaller feature space leads to lower memory and CPU usage.
- âœ… **Less Data Sparsity**: Embeddings allow rare words to share features with similar ones, improving learning from limited data.

---

## ðŸ”„ From Word Embeddings to Model Input

Text examples consist of multiple words. Since each word maps to a K-dimensional vector, we must **aggregate** these vectors into a single, fixed-length vector for model training.

### ðŸ§© Aggregation Methods (Pooling Layers)

| Method       | Description                                                                 |
|--------------|-----------------------------------------------------------------------------|
| **Average**  | Compute element-wise average of all word vectors.                          |
| **Max**      | Takes the maximum value in each vector dimensionâ€”captures standout signals. |
| **Min**      | Captures the smallest valuesâ€”less commonly used.                            |

> ðŸ”„ These are referred to as **pooling layers** in the embedding pipeline.

---

## ðŸ“ˆ Weighted Aggregation with TF-IDF

Not all words contribute equally. To emphasize more meaningful words:

1. **Compute TF-IDF** scores for words in a document.
2. **Multiply each word vector** by its TF-IDF weight.
3. **Apply pooling (e.g., average, max)** on the weighted vectors.

This technique reduces the influence of frequent, generic words and amplifies rare but informative ones.

---

## ðŸ§¬ Dual Pooling (Average + Max)

To capture a **richer semantic representation**:

- Use **both average and max pooling**.
- **Concatenate** the resulting vectors.
- Output dimension becomes **2 Ã— K**.

This approach balances **general context** (average) with **strong signals** (max).

---

## ðŸ“¦ Choosing an Embedding Model

Several pretrained word embeddings are available:

| Model        | Description                      |
|--------------|----------------------------------|
| **Word2Vec** | Trained using context windows    |
| **GloVe**    | Based on word co-occurrence      |
| **FastText** | Includes subword information     |

### ðŸŽ¯ Consider:
- Whether embeddings are appropriate for your task.
- Pretrained vs. custom-trained embeddings.
- Embedding size: commonly **50**, **100**, or **300** dimensions.

---

## ðŸ” Best Practices for Using Word Embeddings

1. **Start simple** with one embedding + basic pooling.
2. **Compare** performance against non-embedding methods.
3. **Iterate**: Adjust embedding size, pooling function, or add weights.
4. **Validate empirically** using model performance metrics.

> ðŸ§ª Thereâ€™s no one-size-fits-all â€” experiment and tune based on your specific use case.

---

## âœ… Summary

Word embeddings offer a powerful way to represent language numerically while preserving meaning. By leveraging **pooling strategies** and **weighting mechanisms**, you can enhance your modelâ€™s understanding of text in a compact and semantically rich form.

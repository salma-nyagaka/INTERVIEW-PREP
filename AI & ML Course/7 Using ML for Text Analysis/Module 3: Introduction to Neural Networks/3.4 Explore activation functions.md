# Activation Functions - Interview Prep Summary

## What Are Activation Functions?

**Purpose**: Enable neural networks to express nonlinear functions by switching linear components on/off
**Key Point**: Without activation functions, neural networks are just linear models

## 3 Main Activation Functions

### 1. Sigmoid Function
**Formula**: σ(z) = 1/(1+e^(-z))
**Range**: 0 to 1
**Shape**: S-curved, smooth transition

**Advantages**:
• Outputs probabilities (0-1 range)
• Good for binary classification (final layer)
• Smooth, differentiable

**Problems**:
• **Vanishing gradient** - Flat tails cause gradients → 0
• **Saturation** - Large weights get stuck in flat regions
• **Training stops** when gradients become tiny

### 2. Tanh (Hyperbolic Tangent)
**Formula**: σ(z) = tanh(z)
**Range**: -1 to 1
**Shape**: Similar to sigmoid but centered at 0

**Advantages**:
• Zero-centered output
• Useful for binary classification

**Problems**:
• **Same gradient issue** as sigmoid
• **Flat tails** prevent effective learning
• **Symmetry problem** - both ends too flat

### 3. ReLU (Rectified Linear Unit) ⭐
**Formula**: σ(z) = max(z, 0)
**Range**: 0 to ∞
**Shape**: Flat at 0, linear for positive values

**Why ReLU Dominates**:
• **No vanishing gradient** in positive region
• **Strong gradients** when activation changes sign
• **Computationally simple** - just max(0, x)
• **Not symmetric** - avoids flat tail problem
• **Sparse activation** - many neurons output 0

## The Gradient Problem Explained

### Why Sigmoid/Tanh Failed in Deep Networks
• **Flat regions** → gradients approach 0
• **Gradient descent stops** → no learning
• **Deep networks suffer most** → gradients multiply through layers

### Why ReLU Solved It
• **Left side flat** (negative inputs → 0)
• **Right side linear** (positive inputs → same value)
• **Strong gradient** in positive region enables learning

## Interview Key Points

### What's the most important activation function?
**ReLU - it solved the vanishing gradient problem that plagued deep networks**

### Why don't we use sigmoid anymore?
**Sigmoid has flat tails where gradients become zero, stopping the learning process**

### What makes ReLU special?
**It's not symmetric - has strong gradients in the positive region, enabling effective training**

### When might you still use sigmoid?
**Final layer for binary classification (outputs probabilities 0-1)**

### What problem did activation functions solve?
**They enable nonlinearity - without them, neural networks are just linear models**

## Quick Comparison

| Function | Range | Main Issue | Best Use |
|----------|-------|------------|----------|
| **Sigmoid** | 0 to 1 | Vanishing gradient | Output layer (binary) |
| **Tanh** | -1 to 1 | Vanishing gradient | Rarely used |
| **ReLU** | 0 to ∞ | None (current standard) | Hidden layers |

## Bottom Line for Interviews
**ReLU revolutionized deep learning by solving the vanishing gradient problem. Use ReLU for hidden layers and sigmoid only for binary classification output layers.**
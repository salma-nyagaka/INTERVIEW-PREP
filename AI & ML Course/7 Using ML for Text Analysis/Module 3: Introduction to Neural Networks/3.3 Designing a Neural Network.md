# Designing a Neural Network - Interview Prep Summary

## Neural Network Architecture = Hyperparameters

**You are the architect** - Design choices control model complexity and performance

## 3 Core Design Components

### 1. Number of Hidden Layers
• **Shallow networks** - 1-2 hidden layers
• **Deep networks** - 10+ hidden layers (no strict threshold)
• **More layers** = More complex patterns, but harder to train

### 2. Number of Nodes per Layer
• **Each layer can have different sizes**
• **More nodes** = More capacity to learn complex relationships
• **Trade-off** - Complexity vs. overfitting risk

### 3. Activation Function
• **Defines how data transforms in each node**
• **Critical choice** - Affects training success

## Activation Functions

### Historical vs. Modern
**Old (rarely used now)**:
• **Sigmoid** - Outputs 0 to 1, becomes flat at extremes
• **Tanh** - Outputs -1 to 1, similar shape to sigmoid

**Current standard**:
• **ReLU (Rectified Linear Unit)** - f(x) = max(0, x)

### Why ReLU Won
• **Gradient problem solved** - Sigmoid/tanh have flat tails where gradients → 0
• **Training stops** when gradients become zero
• **ReLU advantage** - No vanishing gradient in positive region

## Design Strategy

### Start Simple, Iterate
1. **Begin with basic architecture** (e.g., 1 hidden layer, 20 nodes)
2. **Test variations**:
   - Add more layers
   - Add more nodes per layer
3. **Compare performance** and choose best
4. **Repeat process** with winning architecture as new baseline

### Empirical Approach
• **No magic formula** for optimal architecture
• **Experiment and measure** - Use validation performance
• **Standard hyperparameter optimization** applies

## Interview Key Points

### What are neural network hyperparameters?
**Architecture choices: number of layers, nodes per layer, and activation functions**

### How do you choose architecture?
**Start simple and iteratively increase complexity while measuring performance**

### Why is ReLU preferred?
**Prevents vanishing gradient problem that plagued sigmoid/tanh functions**

### What makes a network "deep"?
**Many hidden layers (typically 10+), enabling learning of complex hierarchical patterns**

### Design philosophy?
**Balance between model capacity (complexity) and generalization (avoiding overfitting)**

## Quick Decision Framework
1. **Activation function** → Always use ReLU (unless specific reason not to)
2. **Architecture** → Start with 1-2 layers, 10-50 nodes
3. **Optimization** → Systematic testing of variations
4. **Selection** → Choose based on validation performance

## Bottom Line for Interviews
**Neural network design involves choosing the number of layers, nodes per layer, and activation functions (use ReLU). Start simple and systematically increase complexity while measuring performance to find the optimal architecture.**
# Neural Network Training - Interview Prep Summary

## Training Goal
**Find optimal values for model parameters** (weights) to minimize prediction errors

## Parameter Complexity Challenge
• **Neural networks**: Hundreds to billions of parameters
• **Example**: 20 features, 2 hidden layers (10 nodes each) = 300+ parameters
• **Logistic regression**: Same 20 features = only 20 parameters
• **Challenge**: More parameters = more powerful but harder to train

## Two-Step Training Process

### 1. Forward Propagation
• **Process**: Input → Hidden layers → Output
• **Purpose**: Make predictions by moving forward through network
• **Result**: Get prediction and calculate loss

### 2. Backward Propagation (Backpropagation)
• **Process**: Output → Hidden layers → Input (reverse direction)
• **Purpose**: Calculate how much each weight contributes to the loss
• **Result**: Gradients for updating weights

## Loss Functions (Same as Traditional ML)

### Classification
• **Binary**: Log loss (logistic loss)
• **Multi-class**: Softmax loss (generalized log loss)

### Regression
• **Continuous targets**: Mean Squared Error (MSE)

## Mathematical Tools Required

### Partial Derivatives
• **Purpose**: Measure how function changes with small variable change
• **Also called**: Gradient of function
• **Use**: Determine weight update direction

### Chain Rule
• **Why needed**: Neural networks are composite/nested functions
• **Purpose**: Compute derivatives of nested functions
• **Process**: Take product of each nested function derivative
• **Complexity**: Chain grows with number of layers

## Gradient Descent Variants

### Standard Gradient Descent
• **Use**: Update weights using computed gradients
• **Formula**: weight = weight - learning_rate × gradient

### Stochastic Gradient Descent (SGD)
• **Most common**: Default approach for neural networks
• **Efficiency**: Uses single examples or small batches
• **Advantage**: Much faster than full batch gradient descent
• **Variants**: Multiple SGD variations for different data properties

## Interview Key Points

### Why is neural network training complex?
**Massive number of parameters (millions to billions) compared to traditional models**

### What is backpropagation?
**Algorithm that calculates how much each weight contributes to the loss by moving backward through the network**

### Why use SGD instead of regular gradient descent?
**SGD is more efficient - uses small batches instead of entire dataset for each update**

### What mathematical concepts enable backpropagation?
**Partial derivatives (gradients) and chain rule for composite functions**

### Do neural networks use different loss functions?
**No - same loss functions as traditional ML (log loss, MSE, etc.)**

## Training Flow Summary
1. **Forward pass** → Make prediction
2. **Calculate loss** → Compare prediction to true label
3. **Backward pass** → Compute gradients for each weight
4. **Update weights** → Use SGD to adjust parameters
5. **Repeat** → Until convergence

## Bottom Line for Interviews
**Neural network training uses forward propagation to make predictions, backpropagation to calculate gradients, and stochastic gradient descent to update the massive number of parameters efficiently.**
### When Feature Engineering Is Less Important

There are certain specialized use cases where feature engineering isn't all that important anymore. Two examples are **image recognition** and **language translation**. These are both applications where **deep neural networks** are the state of the art, and they have a special property: they can **automatically learn feature concepts from raw data**.

Before deep learning became common, **concept-based feature engineering** was the norm. Outside of these particular applications, **content-based feature engineering is still critically important**.

---

### The Art of Feature Engineering

In all cases outside deep learning, you must rely on **domain knowledge** and **intuition**. The stage where we map concepts to data representations is very much an **art**—there isn’t a well-defined methodology. Instead, we follow a few guiding principles.

---

### Principles of Effective Feature Engineering

#### 1. Think Like a Scientist

Scientists study how the world works, usually to understand **cause and effect**. Based on your knowledge of the event you're trying to model:

- Identify likely **causal factors**
- Test them **empirically**
- Prefer features that **cause** the outcome, but even **correlated** factors are valid for prediction

#### 2. Seek Input from Other Experts

You are **not the only expert**. Engage others who understand the problem domain, especially:

- **Non-technical experts**, like customer-facing staff
- People with **years of experience** in the field

This also helps you:

- Avoid building **ineligible features** (due to legal/system constraints)
- Save time and resources

#### 3. Be Practical and Strategic

Your imagination is the only limit to the number of features you could build—but:

- Each feature takes **roughly the same time** to implement
- Their **predictiveness varies greatly**
- You won’t know in advance which features will be most valuable

To manage this:

- Create a **feature planning list**
- Get input on each feature’s **expected value**
- Build and **evaluate** iteratively
- Stop when you hit **diminishing returns**

---

### Example: Twitter’s “Who to Follow” Recommendations

Let’s use an example to illustrate this process. Assume the label is:  
**Did the user follow the person recommended?**

#### Step 1: Think Like a Scientist

Ask: *Why do I follow someone on Twitter?*

Avoid abstract reasons like:

- "Do I find the person interesting?"

Instead, look for concepts that are **concrete and codable**, such as:

- **Topic match**: The person tweets about topics I follow
- **Reciprocity**: The person follows me
- **Mutual connections**: People I follow also follow them
- **Popularity**: The person is widely followed

All of these can be turned into **specific, measurable features**.

---

### Planning and Diminishing Returns

Assume you now have a long list of features. In a perfect world, you'd build and test all of them. But realistically:

- **Each feature has a fixed cost**
- **Not all add value**
- **Some will be redundant**

#### Visualizing Returns

If we plot **feature development cost vs. model performance**, we get a curve showing **diminishing returns**:

- Effort 1 builds 50 strong features
- Effort 2 builds 50 weaker or redundant ones
- Effort 1 gives more **performance per unit time**

---

### Final Advice

- Use **collective judgment** to prioritize features
- **Test and evaluate** along the way
- **Stop** when performance gains no longer justify the effort

This is how you balance intuition, collaboration, and empirical testing in the feature engineering process.

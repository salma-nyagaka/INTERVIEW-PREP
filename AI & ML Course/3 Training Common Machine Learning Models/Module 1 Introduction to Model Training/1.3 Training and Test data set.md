## Summary: Training and Test Data Sets

### **The Problem with Overfitting Detection**
Low training error doesn't guarantee good performance on new data due to overfitting. The solution is to test the model on data it hasn't seen during training.

### **Data Splitting Strategy**

**Basic Approach (2-way split):**
- **Training set**: Used to build/train the model
- **Test set**: Used to evaluate final model performance

**Enhanced Approach (3-way split):**
- **Training set**: Build the model
- **Validation set**: Tune and improve the model iteratively
- **Test set**: Final unbiased evaluation (use only once)

### **Why Validation Sets Matter**
- **Problem**: Repeatedly testing and tweaking on the same test set leads to overfitting to that test set
- **Solution**: Use validation set for iterative improvements, reserve test set for final evaluation
- **Process**: Train → Validate → Adjust → Repeat until satisfied → Test once for final assessment

### **Key Benefits**
- **Known labels**: Since test data comes from your original labeled dataset, you can compare predictions to actual values
- **Concrete metrics**: Measure exact performance (e.g., "40% accuracy" or "fraction of emails correctly classified as spam")
- **Unbiased evaluation**: Test set provides honest assessment of generalization ability

### **Evaluation Techniques**
Different methods exist for data partitioning and model analysis, including various loss functions and evaluation metrics that will be covered in later courses.

**Bottom line**: Proper data splitting prevents overfitting and provides reliable performance estimates for real-world deployment.